# VCC Transformer Configuration - Large Model
# Optimized for multiple RTX 3090s with high memory

# Data Configuration
data:
  training_file: "data/adata_Training.h5ad"
  validation_file: "data/pert_counts_Validation.csv"
  output_file: "predictions.h5ad"
  n_highly_variable_genes: 5000
  min_cells_per_perturbation: 10
  max_cells_per_perturbation: 1000
  validation_split: 0.1
  normalize_method: "log1p"
  scale_method: "standard"

# Model Architecture - Large Configuration
model:
  d_model: 1024     # Large hidden dimension
  n_layers: 12      # Deep network
  n_heads: 16       # Many attention heads
  d_ff: 4096        # Large feed-forward
  dropout: 0.1
  layer_norm_eps: 1e-5
  max_seq_length: 5002
  
  # Special tokens
  cls_token_id: 0
  unk_pert_token_id: 1
  unk_pert_probability: 0.1
  
  # Positional encoding
  use_learned_pe: true
  
  # Flash attention
  use_flash_attention: true

# Training Configuration - Optimized for Large Model
training:
  batch_size: 16        # Smaller batch for large model
  learning_rate: 5e-5   # Lower LR for stability
  weight_decay: 0.01
  warmup_steps: 2000    # More warmup for large model
  max_epochs: 150
  gradient_clip_norm: 1.0
  
  # Loss weighting
  reconstruction_weight: 1.0
  classification_weight: 0.3  # Lower weight for stability
  
  # Optimization
  optimizer: "adamw"
  scheduler: "cosine_with_warmup"
  
  # Mixed precision
  use_amp: true
  amp_dtype: "bfloat16"
  
  # Gradient checkpointing for memory
  use_gradient_checkpointing: true
  
  # Compilation
  compile_model: true
  compile_mode: "max-autotune"  # Aggressive optimization

# Multi-GPU Configuration
distributed:
  use_ddp: true
  backend: "nccl"
  find_unused_parameters: false

# Data Loading - Optimized for large model
dataloader:
  num_workers: 12      # More workers for large batches
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 4   # Higher prefetch

# Validation & Evaluation
validation:
  run_validation: true
  val_every_n_epochs: 3
  compute_challenge_metrics: true
  save_predictions: true

# Checkpointing & Logging
checkpointing:
  save_every_n_epochs: 5
  save_top_k: 5
  monitor_metric: "val_combined_loss"
  mode: "min"
  checkpoint_dir: "checkpoints_large"

logging:
  log_every_n_steps: 50
  log_dir: "logs_large"
  use_wandb: true
  wandb_project: "vcc-transformer-large"
  wandb_entity: null

# Hardware Configuration
hardware:
  device: "cuda"
  mixed_precision_backend: "native"
  benchmark_cudnn: true

# Reproducibility
seed: 42
deterministic: false

# Memory Management - Aggressive for large model
memory:
  empty_cache_every_n_steps: 50
  max_memory_usage: 0.95
